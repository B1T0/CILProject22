{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import call\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_full = pd.read_csv('../data_raw/data_train.csv')\n",
    "df_train = pd.read_csv('../data_raw/cross_validation/train_split_4.csv')\n",
    "df_test = pd.read_csv('../data_raw/cross_validation/test_split_4.csv')\n",
    "\n",
    "dic_full = {\n",
    "    'user_id': [int(str(x).partition(\"_\")[0][1:]) for x in df_full['Id']],\n",
    "    'item_id': [int(str(x).partition(\"_\")[2][1:]) for x in df_full['Id']],\n",
    "    #'combined': [(str(x).partition(\"_\")[0][1:],str(x).partition(\"_\")[2][1:]) for x in df['Id']],\n",
    "    'rating': [float(x) for x in df_full['Prediction']],\n",
    "}\n",
    "dic_train = {\n",
    "    'user_id': [int(str(x).partition(\"_\")[0][1:]) for x in df_train['Id']],\n",
    "    'item_id': [int(str(x).partition(\"_\")[2][1:]) for x in df_train['Id']],\n",
    "    'rating': [float(x) for x in df_train['Prediction']],\n",
    "}\n",
    "dic_test = {\n",
    "    'user_id': [int(str(x).partition(\"_\")[0][1:]) for x in df_test['Id']],\n",
    "    'item_id': [int(str(x).partition(\"_\")[2][1:]) for x in df_test['Id']],\n",
    "    'rating': [float(x) for x in df_test['Prediction']],\n",
    "}\n",
    "\n",
    "full_data = pd.DataFrame(dic_full)\n",
    "train_data = pd.DataFrame(dic_train)\n",
    "test_data = pd.DataFrame(dic_test)\n",
    "full_data[:100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "def tune_ALS(train_data, validation_data, maxIter, regParams, ranks):\n",
    "    \"\"\"\n",
    "    Doing Grid Search for tuning the parameters of ALS while keeping track\n",
    "    of the best parameters, the best model and the metrics\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in regParams:\n",
    "            # get ALS model\n",
    "            als = ALS().setMaxIter(maxIter).setRank(rank).setRegParam(reg)\n",
    "            # train ALS model\n",
    "            print(f'Training model with rank {rank} and regularization {reg}')\n",
    "            model = als.fit(train_data)\n",
    "            # evaluate the model by computing the RMSE on the validation data\n",
    "            predictions = model.transform(validation_data)\n",
    "            evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "            rmse = evaluator.evaluate(predictions)\n",
    "            print('{} latent factors and regularization = {}: '\n",
    "                  'validation RMSE is {}'.format(rank, reg, rmse))\n",
    "            if rmse < min_error:\n",
    "                min_error = rmse\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and '\n",
    "          'regularization = {} with RSME = {}'.format(best_rank, best_regularization, min_error))\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tune_ALS(df_train, df_test, maxIter = 15, regParams = [0.05, 0.1, 0.2], ranks = [5, 10, 15, 20, 25, 30])\n",
    "print('Training complete')\n",
    "pred = model.transform(df_test)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "                                            labelCol=\"rating\",\n",
    "                                            predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(pred)\n",
    "print('RSME: {}' .format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict ratings\n",
    "sample_sub = pd.read_csv(\"../data_raw/sampleSubmission.csv\")\n",
    "to_predict = []\n",
    "for i, cell_id in enumerate(sample_sub.Id):\n",
    "    row, col = cell_id.split(\"_\")\n",
    "    to_predict.append((int(row[1:]), int(col[1:])))\n",
    "df_sub = spark.createDataFrame(to_predict, [\"user\", \"item\"])\n",
    "\n",
    "ensemble = 5\n",
    "average = np.asarray([0.0] * len(to_predict))\n",
    "for i in range(ensemble):\n",
    "    als = ALS(seed=i).setMaxIter(15).setRank(30).setRegParam(0.1)\n",
    "    print(f'Training model {i+1}/{ensemble}')\n",
    "    model = als.fit(df_full)\n",
    "    predictions = model.transform(df_sub)\n",
    "    pandas_df = predictions.toPandas()\n",
    "    pandas_df = pandas_df.sort_values(by=['item', 'user'], ascending=True)\n",
    "    average += np.asarray(pandas_df['prediction'].values.tolist())\n",
    "    print(pandas_df[:3])\n",
    "average = average / ensemble\n",
    "\n",
    "print(pandas_df[:5])\n",
    "print(predictions.show())\n",
    "\n",
    "# write to csv\n",
    "sample_sub.Prediction = average\n",
    "sample_sub.to_csv(\"../data/als.csv\", index=False)\n",
    "sample_sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('web3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2522a7ac3b60f852504f7e2e80aa0b42867087115d2f979bf28cdf9a03dea8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
